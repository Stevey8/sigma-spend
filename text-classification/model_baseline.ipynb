{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/name_price.csv\", header=0) \n",
    "# df.head()\n",
    "texts = df['name'].tolist()\n",
    "labels = df['category'].tolist()\n",
    "# print(texts[:5])\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# generate SBERT embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and accurate; other possible pretrained models: https://www.sbert.net/docs/pretrained_models.html\n",
    "X_embeddings = model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bills' 'entertainment' 'grocery' 'health & wellness' 'restaurant'\n",
      " 'shopping' 'transportation']\n"
     ]
    }
   ],
   "source": [
    "# encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "# print(y)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            bills       1.00      1.00      1.00         3\n",
      "    entertainment       0.89      0.89      0.89        18\n",
      "          grocery       0.50      0.17      0.25         6\n",
      "health & wellness       0.67      0.80      0.73         5\n",
      "       restaurant       0.87      0.90      0.88        29\n",
      "         shopping       0.78      0.88      0.82         8\n",
      "   transportation       0.80      0.89      0.84         9\n",
      "\n",
      "         accuracy                           0.83        78\n",
      "        macro avg       0.79      0.79      0.77        78\n",
      "     weighted avg       0.82      0.83      0.82        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf = LogisticRegression(max_iter=1000,class_weight='balanced', n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shopping\n"
     ]
    }
   ],
   "source": [
    "new_text = [\"something\"]\n",
    "new_embedding = model.encode(new_text)\n",
    "predicted_class = le.inverse_transform(clf.predict(new_embedding))\n",
    "print(predicted_class[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "# soft prediction \n",
    "def soft_pred(name, margin=0.2):\n",
    "    embedding = model.encode([name])\n",
    "    proba = clf.predict_proba(embedding)[0]\n",
    "    \n",
    "    # Get top two indices and their probabilities\n",
    "    top2_idx = np.argsort(proba)[-2:][::-1]\n",
    "    p1, p2 = proba[top2_idx[0]], proba[top2_idx[1]]\n",
    "    c1, c2 = le.classes_[top2_idx[0]], le.classes_[top2_idx[1]]\n",
    "    \n",
    "    if p1 - p2 >= margin:\n",
    "        return f\"→ Predicted category: {c1} ({p1:.2f})\"\n",
    "    else:\n",
    "        return f\"→ Ambiguous. Top 2: {c1} ({p1:.2f}), {c2} ({p2:.2f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Ambiguous. Top 2: restaurant (0.23), shopping (0.23)\n",
      "→ Predicted category: grocery (0.50)\n",
      "→ Predicted category: transportation (0.57)\n",
      "→ Ambiguous. Top 2: transportation (0.44), grocery (0.30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(soft_pred(\"Shell\"))\n",
    "print(soft_pred(\"7 Eleven\"))\n",
    "print(soft_pred(\"Subway\"))\n",
    "print(soft_pred(\"metro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Top 5 predictions: transportation (0.57), restaurant (0.17), grocery (0.11), entertainment (0.06), shopping (0.05)\n"
     ]
    }
   ],
   "source": [
    "def print_top_5_probs(name):\n",
    "    embedding = model.encode([name])\n",
    "    proba = clf.predict_proba(embedding)[0]\n",
    "    \n",
    "    top5_idx = np.argsort(proba)[-5:][::-1]\n",
    "    \n",
    "    result = [f\"{le.classes_[i]} ({proba[i]:.2f})\" for i in top5_idx]\n",
    "    return \"→ Top 5 predictions: \" + \", \".join(result)\n",
    "\n",
    "print(print_top_5_probs(\"Subway\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
